# MedObsMind - Enhanced User Profile System

## üéØ Overview

MedObsMind implements a comprehensive user profile system that captures detailed user attributes to:
1. **Personalize AI responses** based on user context
2. **Optimize resource usage** through intelligent pooling
3. **Identify proper use cases** for each user type
4. **Reuse common LLM resources** efficiently
5. **Provide role-appropriate content** and features

---

## üë§ User Profile Attributes

### Core Identity
```json
{
  "user_id": "uuid",
  "email": "doctor@hospital.com",
  "phone": "+91-9876543210",
  "name": "Dr. Rajesh Sharma",
  "username": "dr_rajesh_sharma",
  "profile_picture": "url_to_image"
}
```

### Professional Status
```json
{
  "status": "professional",  // or "public"
  "role": "doctor",           // doctor, student, nurse, researcher, patient
  "sub_role": "cardiologist", // specialty or specific role
  "
_level": "senior",   // junior, mid-level, senior, expert
  "verified": true,           // professional verification status
  "license_number": "MCI-123456",
  "registration_year": 2010
}
```

### Demographics
```json
{
  "age": 35,
  "age_group": "30-40",       // for anonymization
  "gender": "male",           // male, female, other, prefer_not_to_say
  "date_of_birth": "1989-03-15",
  "location": {
    "country": "India",
    "state": "Maharashtra",
    "city": "Mumbai",
    "district": "Mumbai Suburban",
    "pincode": "400001"
  },
  "languages": ["English", "Hindi", "Marathi"],
  "preferred_language": "Hindi"
}
```

### Education & Qualifications
```json
{
  "qualifications": [
    {
      "degree": "MBBS",
      "institution": "Grant Medical College, Mumbai",
      "year": 2013,
      "specialization": null
    },
    {
      "degree": "MD",
      "institution": "AIIMS, Delhi",
      "year": 2016,
      "specialization": "Cardiology"
    },
    {
      "degree": "DM",
      "institution": "PGIMER, Chandigarh",
      "year": 2019,
      "specialization": "Interventional Cardiology"
    }
  ],
  "current_education": {
    "pursuing": false,
    "degree": null,
    "year": null
  },
  "certifications": [
    "ACLS (Advanced Cardiac Life Support)",
    "ECMO Specialist",
    "Fellow of American College of Cardiology (FACC)"
  ]
}
```

### Professional Experience
```json
{
  "experience": {
    "total_years": 10,
    "current_position": "Senior Consultant Cardiologist",
    "current_hospital": "Apollo Hospitals, Mumbai",
    "department": "Cardiology & Cardiac Surgery",
    "areas_of_expertise": [
      "Interventional Cardiology",
      "Acute Coronary Syndrome",
      "Heart Failure Management",
      "Cardiac Critical Care"
    ],
    "procedures_performed": {
      "angioplasty": 500,
      "pacemaker_implantation": 100,
      "ecmo": 50
    }
  }
}
```

### Interests & Hobbies
```json
{
  "hobbies": [
    "Medical research",
    "Teaching medical students",
    "Playing cricket",
    "Reading medical journals",
    "Photography"
  ],
  "interests": [
    "Cardiovascular research",
    "Medical AI and technology",
    "Public health awareness",
    "Medical education innovation"
  ],
  "research_interests": [
    "AI in cardiac imaging",
    "Predictive analytics for heart failure",
    "Telemedicine in cardiology"
  ]
}
```

### Usage Patterns
```json
{
  "usage": {
    "primary_use_case": "clinical_decision_support",
    "secondary_use_cases": ["teaching", "research"],
    "typical_scenarios": [
      "ICU patient monitoring",
      "Emergency cardiac cases",
      "Teaching rounds",
      "Case discussions"
    ],
    "device_preference": "mobile",  // mobile, web, tablet
    "average_sessions_per_day": 5,
    "peak_usage_time": "09:00-17:00",
    "weekend_usage": true
  }
}
```

### Learning & Preferences
```json
{
  "learning_profile": {
    "aetcom_modules_completed": ["Module 1", "Module 2"],
    "competencies_achieved": ["K", "KH", "S"],
    "preferred_learning_style": "case_based",  // visual, case_based, theory, hands_on
    "areas_for_improvement": ["pediatric_cardiology"]
  },
  "ai_preferences": {
    "explanation_detail": "detailed",  // brief, moderate, detailed, expert
    "citation_style": "references",     // inline, references, none
    "language_mix": "code_mixing",      // pure_english, pure_hindi, code_mixing
    "voice_enabled": true,
    "streaming_responses": true
  }
}
```

---

## üè∑Ô∏è User Status Types

### 1. PUBLIC Status

**Definition:** General users without professional medical credentials

**Characteristics:**
- Age: Any
- Qualification: Non-medical or students (pre-clinical)
- Use Case: Learning, health awareness, general information

**Access Level:**
- Basic health information
- Educational content
- Symptom checker (non-diagnostic)
- General wellness advice
- Public health information

**LLM Behavior:**
- Simple, layman language
- No medical jargon
- Disclaim professional advice
- Encourage doctor consultation
- General health education

**Example Users:**
- Patients and their families
- Health-conscious individuals
- Pre-medical students
- Health bloggers/influencers
- General public

**Profile Example:**
```json
{
  "user_id": "pub_123",
  "name": "Amit Kumar",
  "status": "public",
  "age": 28,
  "qualification": "B.Tech Computer Science",
  "hobbies": ["Fitness", "Yoga", "Health blogs"],
  "use_case": "personal_health_awareness",
  "access_level": "basic"
}
```

### 2. PROFESSIONAL Status

**Definition:** Verified medical professionals with credentials

**Sub-Types:**

#### a) Medical Students (Professional-in-Training)
```json
{
  "status": "professional",
  "role": "student",
  "sub_role": "mbbs_student",
  "year": 3,
  "college": "Grant Medical College",
  "age": 21,
  "qualifications": ["12th PCB", "NEET"],
  "hobbies": ["Clinical rotation observation", "Medical quiz"],
  "use_case": "education_learning",
  "access_level": "educational"
}
```

**Access:**
- Full educational content
- Case-based learning
- AETCOM modules
- Exam preparation
- Simulation scenarios
- No real patient access

**LLM Behavior:**
- Educational explanations
- Step-by-step reasoning
- Exam-oriented answers
- MBBS curriculum aligned
- Encouraging feedback

#### b) Residents/Interns
```json
{
  "status": "professional",
  "role": "resident",
  "sub_role": "medicine_resident",
  "year": 2,
  "age": 26,
  "qualifications": ["MBBS", "MD pursuing"],
  "hobbies": ["Journal reading", "Case discussions"],
  "use_case": "clinical_training",
  "access_level": "intermediate"
}
```

**Access:**
- Clinical decision support
- Real patient cases (supervised)
- Learning from real data
- Research access
- NBEMS preparation

**LLM Behavior:**
- Detailed clinical reasoning
- Differential diagnosis approach
- Evidence-based recommendations
- Learning-focused explanations

#### c) Doctors (Attending/Consultants)
```json
{
  "status": "professional",
  "role": "doctor",
  "sub_role": "cardiologist",
  "experience_years": 10,
  "age": 35,
  "qualifications": ["MBBS", "MD", "DM"],
  "hobbies": ["Research", "Teaching", "Golf"],
  "use_case": "clinical_decision_support",
  "access_level": "full_clinical"
}
```

**Access:**
- Full clinical features
- All patient data (assigned)
- AI decision support
- Research tools
- Administrative functions

**LLM Behavior:**
- Expert-level discussion
- Quick, actionable insights
- Evidence citations
- Differential considerations
- Time-efficient responses

#### d) Researchers
```json
{
  "status": "professional",
  "role": "researcher",
  "sub_role": "clinical_researcher",
  "age": 40,
  "qualifications": ["MBBS", "MD", "PhD"],
  "hobbies": ["Data analysis", "Publishing papers"],
  "use_case": "research_analytics",
  "access_level": "research"
}
```

**Access:**
- Anonymized datasets
- Population analytics
- Trend analysis
- Research tools
- Data export (with approval)

**LLM Behavior:**
- Statistical insights
- Research methodology
- Literature references
- Data interpretation
- Hypothesis generation

#### e) Nurses
```json
{
  "status": "professional",
  "role": "nurse",
  "sub_role": "icu_nurse",
  "experience_years": 7,
  "age": 30,
  "qualifications": ["B.Sc Nursing", "Critical Care Nursing"],
  "hobbies": ["Patient care", "Continuous education"],
  "use_case": "patient_monitoring",
  "access_level": "nursing"
}
```

**Access:**
- Vitals entry
- Patient monitoring
- Alert viewing
- Care protocols
- Limited AI recommendations

**LLM Behavior:**
- Nursing-focused advice
- Practical care tips
- Protocol reminders
- Patient safety focus
- Clear action items

---

## üîÑ Resource Pooling & LLM Optimization

### Common Resource Pool Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         MedObsMind LLM Resource Pool        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   dsquaremedicalmodel (Base)       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Shared across all users        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ 8B parameters                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ 4.5 GB quantized              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Context Pools (Reusable)         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Medical knowledge base         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ ICMR guidelines                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Drug formulary                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Scoring systems                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   User-Specific Adaptations        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Professional: Clinical focus   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Student: Educational focus     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Public: Simple explanations    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Prompt Templates (Cached)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Doctor templates               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Student templates              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Public templates               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Resource Reuse Strategy

#### 1. Single Model, Multiple Contexts

**Instead of:**
```
‚ùå Different model for each user type (inefficient)
- Doctor model: 4.5 GB
- Student model: 4.5 GB
- Public model: 4.5 GB
Total: 13.5 GB memory
```

**We Use:**
```
‚úÖ Single model + Context adaptation (efficient)
- Base model: 4.5 GB
- Context cache: 500 MB
- Prompt templates: 10 MB
Total: ~5 GB memory
```

#### 2. Shared Knowledge Base (RAG)

**Common Resources:**
```python
# All users share the same knowledge base
knowledge_base = {
    "medical_guidelines": "/data/guidelines/",  # ICMR, AIIMS
    "drug_formulary": "/data/drugs/",           # Indian drugs
    "scoring_systems": "/data/scores/",         # NEWS2, qSOFA
    "protocols": "/data/protocols/",            # Clinical protocols
}

# Context adapted per user
def get_context(user_profile, query):
    base_context = retrieve_from_knowledge_base(query)
    
    if user_profile.status == "professional":
        if user_profile.role == "doctor":
            return base_context  # Full technical details
        elif user_profile.role == "student":
            return simplify_for_learning(base_context)
    else:  # public
        return simplify_for_layman(base_context)
```

#### 3. Prompt Template Caching

**Cached Templates:**
```python
# Templates cached in memory, reused for all users
PROMPT_TEMPLATES = {
    "professional_doctor": """
        You are DDMA, an AI medical assistant for professional doctors.
        Provide expert-level clinical insights with evidence citations.
        User: {name}, {role}, {specialty}, {experience_years} years
        Context: {medical_context}
        Query: {user_query}
    """,
    
    "professional_student": """
        You are DDMA, an AI tutor for medical students.
        Explain concepts clearly with learning objectives.
        Student: {name}, Year {year}, {college}
        Learning goal: {learning_objective}
        Query: {user_query}
    """,
    
    "public": """
        You are DDMA, a health information assistant for the public.
        Use simple language, avoid jargon, disclaim professional advice.
        User: {name}, Age {age}
        Health interest: {interest}
        Query: {user_query}
    """
}

# Reuse template, just fill in user-specific details
def generate_prompt(user_profile, query):
    template_key = f"{user_profile.status}_{user_profile.role}"
    template = PROMPT_TEMPLATES.get(template_key, PROMPT_TEMPLATES["public"])
    
    return template.format(
        name=user_profile.name,
        role=user_profile.role,
        specialty=user_profile.get("specialty"),
        experience_years=user_profile.get("experience_years"),
        age=user_profile.age,
        user_query=query
    )
```

#### 4. User-Specific Adjustments

**Without Retraining:**
```python
def adjust_response(base_response, user_profile):
    """
    Adjust LLM response based on user profile
    WITHOUT retraining the model
    """
    
    # Adjust language complexity
    if user_profile.status == "public":
        base_response = simplify_medical_terms(base_response)
    
    # Adjust detail level
    if user_profile.ai_preferences.explanation_detail == "brief":
        base_response = summarize(base_response, max_length=100)
    elif user_profile.ai_preferences.explanation_detail == "detailed":
        base_response = add_detailed_explanation(base_response)
    
    # Adjust language
    if user_profile.preferred_language == "Hindi":
        # Medical terms stay in English, rest in Hindi
        base_response = translate_with_medical_preservation(base_response, "hindi")
    
    # Add appropriate disclaimers
    if user_profile.status == "public":
        base_response += "\n\n‚ö†Ô∏è This is for informational purposes only. Please consult a doctor for medical advice."
    
    return base_response
```

#### 5. Efficient Context Switching

**User Session Management:**
```python
class LLMResourcePool:
    def __init__(self):
        self.model = load_model("dsquaremedicalmodel")  # Load once
        self.knowledge_base = load_knowledge_base()     # Load once
        self.prompt_cache = {}                          # Cache prompts
        self.active_sessions = {}                       # Track sessions
    
    def get_response(self, user_id, query):
        """
        Reuse loaded model and resources
        Only switch context, not the model
        """
        # Get user profile
        user_profile = self.get_user_profile(user_id)
        
        # Check if session exists
        if user_id in self.active_sessions:
            conversation_history = self.active_sessions[user_id]
        else:
            conversation_history = []
        
        # Build context (reuse knowledge base)
        context = self.knowledge_base.retrieve(query, user_profile)
        
        # Build prompt (reuse template)
        prompt = self.build_prompt(user_profile, query, context, conversation_history)
        
        # Generate response (reuse model)
        response = self.model.generate(prompt)
        
        # Adjust response (post-processing based on profile)
        adjusted_response = self.adjust_for_user(response, user_profile)
        
        # Update session
        self.active_sessions[user_id].append({
            "query": query,
            "response": adjusted_response
        })
        
        return adjusted_response
```

### Memory & Performance Optimization

**Resource Usage:**
```
Single Instance:
‚îú‚îÄ‚îÄ Model (loaded once): 4.5 GB
‚îú‚îÄ‚îÄ Knowledge base: 500 MB
‚îú‚îÄ‚îÄ Prompt cache: 10 MB
‚îú‚îÄ‚îÄ Active sessions (100 users): 100 MB
‚îî‚îÄ‚îÄ Total: ~5 GB

vs Multiple Models:
‚îú‚îÄ‚îÄ Doctor model: 4.5 GB
‚îú‚îÄ‚îÄ Student model: 4.5 GB
‚îú‚îÄ‚îÄ Public model: 4.5 GB
‚îî‚îÄ‚îÄ Total: 13.5 GB (3x more!)
```

**Performance:**
- ‚úÖ 3x memory savings
- ‚úÖ Faster response (no model switching)
- ‚úÖ Consistent quality across users
- ‚úÖ Easy to update (single model)
- ‚úÖ Scalable to 1000s of users

---

## üé≠ Use Case Identification

### Automatic Use Case Detection

```python
def identify_use_case(user_profile, query, context):
    """
    Automatically identify the proper use case
    based on user profile and query
    """
    
    # Professional users
    if user_profile.status == "professional":
        if user_profile.role == "doctor":
            if "patient" in query.lower():
                return "clinical_decision_support"
            elif "research" in query.lower():
                return "research_query"
            elif "teach" in query.lower():
                return "medical_education"
        
        elif user_profile.role == "student":
            if "exam" in query.lower():
                return "exam_preparation"
            elif "case" in query.lower():
                return "case_based_learning"
            elif "explain" in query.lower():
                return "concept_explanation"
        
        elif user_profile.role == "nurse":
            if "vitals" in query.lower():
                return "vitals_monitoring"
            elif "protocol" in query.lower():
                return "care_protocol"
    
    # Public users
    else:
        if "symptom" in query.lower():
            return "symptom_checker"
        elif "medicine" in query.lower():
            return "medication_info"
        else:
            return "general_health_info"
```

### Use Case-Specific Responses

```python
USE_CASE_HANDLERS = {
    "clinical_decision_support": {
        "temperature": 0.3,        # Low (consistent)
        "max_tokens": 500,
        "tools": ["vitals_lookup", "guideline_search", "drug_check"],
        "tone": "professional",
        "format": "structured",
        "citations": True
    },
    
    "exam_preparation": {
        "temperature": 0.5,        # Medium (educational)
        "max_tokens": 800,
        "tools": ["question_bank", "explanation_engine"],
        "tone": "educational",
        "format": "step_by_step",
        "citations": True
    },
    
    "symptom_checker": {
        "temperature": 0.7,        # Higher (empathetic)
        "max_tokens": 300,
        "tools": ["symptom_database"],
        "tone": "empathetic",
        "format": "simple",
        "citations": False,
        "disclaimer": "‚ö†Ô∏è Please consult a doctor for proper diagnosis"
    }
}
```

---

## üìä Profile-Based LLM Configuration

### Configuration Matrix

| User Profile | Temperature | Max Tokens | Detail Level | Tools | Citations |
|--------------|-------------|------------|--------------|-------|-----------|
| **Doctor (Senior)** | 0.2 | 300 | Brief | All | Yes |
| **Doctor (Junior)** | 0.3 | 500 | Detailed | All | Yes |
| **Resident** | 0.4 | 600 | Learning | Most | Yes |
| **Student (Final Year)** | 0.5 | 800 | Educational | Limited | Yes |
| **Student (Early)** | 0.6 | 1000 | Basic | None | Optional |
| **Nurse** | 0.4 | 400 | Practical | Vitals only | Optional |
| **Researcher** | 0.3 | 1000 | Technical | Analytics | Always |
| **Public** | 0.7 | 400 | Simple | None | No |

### Dynamic Configuration

```python
def get_llm_config(user_profile, use_case):
    """
    Dynamically configure LLM based on user profile
    """
    config = {
        "model": "dsquaremedicalmodel",
        "temperature": 0.5,  # Default
        "max_tokens": 500,
        "stream": user_profile.ai_preferences.streaming_responses
    }
    
    # Adjust based on professional status
    if user_profile.status == "professional":
        if user_profile.role == "doctor":
            config["temperature"] = 0.2  # More consistent
            config["max_tokens"] = 300   # Concise
            config["tools"] = ["vitals", "guidelines", "drugs", "scores"]
        
        elif user_profile.role == "student":
            config["temperature"] = 0.5  # More exploratory
            config["max_tokens"] = 800   # More detailed
            config["tools"] = ["education", "cases", "exams"]
    
    else:  # public
        config["temperature"] = 0.7  # More conversational
        config["max_tokens"] = 400
        config["tools"] = []  # No clinical tools
    
    # Adjust based on experience
    if user_profile.get("experience_years", 0) > 10:
        config["max_tokens"] = 200  # Expert needs brief answers
        config["detail_level"] = "expert"
    
    # Adjust based on preferences
    if user_profile.ai_preferences.explanation_detail == "brief":
        config["max_tokens"] = min(config["max_tokens"], 200)
    
    return config
```

---

## üîê Privacy & Data Usage

### Profile Data Usage

**What's Used:**
- ‚úÖ Age group (not exact age)
- ‚úÖ Role and status
- ‚úÖ Qualifications (education level)
- ‚úÖ Professional experience (years)
- ‚úÖ Language preferences
- ‚úÖ AI preferences

**What's NOT Used:**
- ‚ùå Personal identifiers (name, email, phone)
- ‚ùå Exact location (only state/district)
- ‚ùå Hobbies (unless relevant to query)
- ‚ùå Personal health information

**Anonymization:**
```python
def anonymize_profile_for_llm(user_profile):
    """
    Create anonymized version for LLM context
    """
    return {
        "role": user_profile.role,
        "status": user_profile.status,
        "age_group": user_profile.age_group,
        "qualification_level": extract_level(user_profile.qualifications),
        "experience_level": user_profile.experience_level,
        "preferred_language": user_profile.preferred_language,
        "location_state": user_profile.location.state  # State only
        # NO name, email, phone, exact age, exact location
    }
```

---

## üöÄ Implementation Example

### Complete Flow

```python
# 1. User logs in
user = authenticate("doctor@hospital.com", "password")

# 2. Load user profile
profile = load_user_profile(user.id)
# {
#   "status": "professional",
#   "role": "doctor",
#   "age": 35,
#   "qualifications": ["MBBS", "MD"],
#   "experience_years": 10,
#   "hobbies": ["Research", "Teaching"],
#   "preferred_language": "Hindi"
# }

# 3. User makes a query
query = "Patient has NEWS2 score 8 with fever. What should I do?"

# 4. Identify use case
use_case = identify_use_case(profile, query)
# Result: "clinical_decision_support"

# 5. Get LLM config for this user + use case
llm_config = get_llm_config(profile, use_case)
# {
#   "temperature": 0.2,
#   "max_tokens": 300,
#   "tools": ["vitals", "guidelines"],
#   "detail_level": "expert"
# }

# 6. Build prompt using cached template
prompt = build_prompt(profile, query, use_case)
# "You are DDMA for Dr. [Anonymous], senior doctor with 10 years experience..."

# 7. Generate response from shared model
response = llm_pool.generate(prompt, llm_config)

# 8. Adjust response based on profile
adjusted_response = adjust_response(response, profile)
# - Keep medical terms (doctor)
# - Brief format (senior)
# - Hindi medical terms if preferred
# - Evidence citations included

# 9. Return to user
return adjusted_response
```

---

## üìä Database Schema

### User Profile Table

```sql
CREATE TABLE user_profiles (
    user_id UUID PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    phone VARCHAR(20),
    name VARCHAR(255),
    username VARCHAR(100) UNIQUE,
    profile_picture_url TEXT,
    
    -- Status & Role
    status VARCHAR(20) NOT NULL,  -- 'public', 'professional'
    role VARCHAR(50),              -- 'doctor', 'student', 'nurse', etc.
    sub_role VARCHAR(100),
    experience_level VARCHAR(20),  -- 'junior', 'mid', 'senior', 'expert'
    verified BOOLEAN DEFAULT FALSE,
    license_number VARCHAR(100),
    registration_year INTEGER,
    
    -- Demographics
    age INTEGER,
    age_group VARCHAR(10),         -- '20-30', '30-40', etc.
    gender VARCHAR(20),
    date_of_birth DATE,
    location_country VARCHAR(100),
    location_state VARCHAR(100),
    location_city VARCHAR(100),
    location_district VARCHAR(100),
    location_pincode VARCHAR(10),
    languages JSONB,               -- ['English', 'Hindi']
    preferred_language VARCHAR(50),
    
    -- Qualifications
    qualifications JSONB,          -- Array of degree objects
    current_education JSONB,
    certifications JSONB,
    
    -- Experience
    experience JSONB,              -- Professional experience details
    
    -- Interests
    hobbies JSONB,                 -- Array of hobbies
    interests JSONB,               -- Array of interests
    research_interests JSONB,
    
    -- Usage
    usage_patterns JSONB,
    
    -- Learning
    learning_profile JSONB,
    
    -- AI Preferences
    ai_preferences JSONB,
    
    -- Metadata
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP,
    active BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_user_status_role ON user_profiles(status, role);
CREATE INDEX idx_user_location ON user_profiles(location_state, location_city);
CREATE INDEX idx_user_age_group ON user_profiles(age_group);
```

---

## üéØ Benefits Summary

### For Users
1. **Personalized Experience** - AI adapts to your level
2. **Relevant Content** - Based on role and interests
3. **Efficient Learning** - Matches your education level
4. **Language Comfort** - Preferred language support
5. **Privacy** - Only relevant data used

### For System
1. **Resource Efficiency** - 3x memory savings
2. **Single Model** - Easy to maintain
3. **Consistent Quality** - Same model for all
4. **Scalable** - Handle 1000s of users
5. **Fast Responses** - No model switching

### For Administrators
1. **User Insights** - Understand user demographics
2. **Usage Analytics** - See how different profiles use system
3. **Targeted Improvements** - Focus on user needs
4. **Compliance** - Track professional verification
5. **Resource Planning** - Optimize based on usage patterns

---

## üîÑ Profile Updates

### Dynamic Profile Evolution

```python
# Profile updates based on usage
def update_profile_from_usage(user_id):
    """
    System learns from user behavior
    """
    usage_data = get_usage_analytics(user_id)
    profile = get_user_profile(user_id)
    
    # Update primary use case
    if usage_data["most_common_queries"] == "clinical":
        profile.usage.primary_use_case = "clinical_decision_support"
    
    # Update expertise areas
    if usage_data["frequent_topics"]:
        profile.areas_of_expertise.extend(usage_data["frequent_topics"])
    
    # Update preferred explanation detail
    if usage_data["avg_response_length"] < 200:
        profile.ai_preferences.explanation_detail = "brief"
    
    save_user_profile(profile)
```

---

**Document Version:** 1.0  
**Last Updated:** 2026-02-06  
**Next Review:** Quarterly
